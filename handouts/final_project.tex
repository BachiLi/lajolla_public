\input{preamble}

\begin{document}

\header{3}{Final Project}

The final project will be a project of your choice. You can implement some non-trivial rendering algorithms, or render some interesting scenes in lajolla or your own renderer. It can also be a rendering-related research project with a new algorithm or system. 
Below we provide some options for you to think about, but feel free to come up with your own ones.

\paragraph{Grading.} 
The project will be graded based on two aspects: technical sophistication and artistic sophistication.
You can have a project that does not have much technical component, but it renders beautiful images.
Alternatively, you can have a project that produces programmer arts, but is highly technical. 
To encourage people to pursue ambitious projects, the more ambitious the project is, the less requirement that is to the completness of the project. 
For example, if you aim to work on a research idea that is novel enough to be published at SIGGRAPH, we will not judge you using the standard of a technical paper.
It is sufficient to show your work and the initial prototypes and experiments you develop, even if the initial result does not show practical benefit yet.

\paragraph{Teaming up.}
For the final project, you can either do it alone or form a group of two people. We do expect that you'll do more difficult projects with two people, but only slightly so. As every software developer know, more people doesn't immediately lead to faster progress, so be careful. What teamwork buys you is probably more capacity to brainstorm ideas and the friendship.

For the research type projects, to avoid conflicts, if multiple groups want to work on the same project, we will consider merging the groups. 

\paragraph{Logistics.}
Before 2/26, please let us know what you plan to do for the final project by submitting a 1-2 pages brief report to Canvas.
Schedule a chat with us if you have any question. 
We will have a checkpoint at 3/11: send us a brief progress report in a few pages on Canvas (ideally provide results and images) describing what you did and what you plan to do next.

\section{Implementation ideas}

Following are projects that are less novel, but could produce cool images. 
They are usually about the implementation of the papers we talked about in the class.
I did not sort them in terms of difficulty.

\paragraph{Energy-preserving microfacet BSDFs.} 
Implement Heitz et al.'s multiple-scattering microfacet BSDF~\cite{Heitz:2016:MMB} in lajolla.
Alternatively, implement the position-free variant~\cite{Wang:2022:PMC}.
As a bonus, compare them to the \href{https://blog.selfshadow.com/publications/s2017-shading-course/imageworks/s2017_pbs_imageworks_slides_v2.pdf}{energy compensation technique} introduced by Kulla and Conty, and show the pros and cons.

\paragraph{Normal map and displacement map filtering.}
Implement LEAN~\cite{Olano:2010:LM} and use it for rendering high resolution normal map in lajolla.
Compare it to a high sample count reference.
When does it work well and when does it fail?
As a bonus, implement LEADR~\cite{Dupuy:2013:LEA} for filtering displacement map.
First you will need to implement displacement mapping in lajolla.
This can be done in a preprocessing pass to convert meshes with displacement map textures into tesselated mesh, or in an on-the-fly manner~\cite{Thonat:2021:TDM}.
Alternatively, implement Yan et al.'s normal map filtering technique~\cite{Yan:2014:RGH} in lajolla. 

\paragraph{Layered BSDF.}
Implement the position-free layered BSDF simulator from Guo et al.~\cite{Guo:2018:PMC} in lajolla.
As a starting point, maybe start with a two layer BSDF with no volumetric scattering in between.
Implementing the unidirectional version is good enough.
As a bonus, read Gamboa et al.'s work~\cite{Gamboa:2020:ETE} and implement their sampling strategy.

\paragraph{Hair BCSDF.}
Implement Marschner's hair scattering model~\cite{Marschner:2003:LSH} in lajolla.
You'll have to implement a ray-curve intersection routine (it is acceptable to reuse the one from Embree) and a hair BCSDF.
You may want to read Matt Pharr's \href{https://www.pbrt.org/hair.pdf}{note} on implement hair rendering in pbrt.

\paragraph{Thin-film iridescence BSDF.}
Implement Belcour's thin-film iridescence BSDF~\cite{Belcour:2017:PEM} in lajolla.
Feel free to look at Belcour's source code.

\paragraph{Faster null-scattering using space partitioning.}
Implement Yue et al.'s kd-tree data structure~\cite{Yue:2010:UAS} or uniform grid~\cite{Yue:2011:TOS} for better upper bound of the extinction coefficients. As a bonus, implement progressive null tracking~\cite{Misso:2023:PNT}. 

\paragraph{Emissive volumes.}
Add emissive volumes to your Homework 2 code. You might want to read Pixar's volume sampling paper from Villemin and Hery~\cite{Villemin:2013:PIF}.

\paragraph{Efficient transmittance estimation.}
Implement Kettunen et al.'s unbiased ray marching estimator~\cite{Kettunen:2021:URT}.
You can extend your volumetric path tracer in homework 2 for this.
Replace your ratio tracker with the new ray marching estimator.
Compare to the ratio tracker and analyze their variance reduction properties.

\paragraph{SGGX.}
Implement the SGGX microflake phase function~\cite{Heitz:2015:SMD} in lajolla. Use it for rendering something like anisotropic volume appearance or level of details.

\paragraph{BSSRDF.}
Implement a BSSRDF in lajolla.
You can use the one from Christensen and Burley~\cite{Christensen:2015:ARP}.
For importance sampling the BSSRDF, check out King et al.'s \href{https://pdfs.semanticscholar.org/90da/5211ce2a6f63d50b8616736c393aaf8bf4ca.pdf}{talk}.
You are encouraged to read pbrt's \href{https://www.pbr-book.org/3ed-2018/Light_Transport_II_Volume_Rendering/Sampling_Subsurface_Reflection_Functions}{code}.

\paragraph{Single scattering.}
Implement Chen et al.'s 1D Min-Max mipmap shadow mapping technique for rendering single scattering volumes~\cite{Chen:2011:RVS}.
You don't have to implement it on GPUs or show real-time performance.

\paragraph{Differentiable rendering.}
Implement edge sampling or warped area sampling in \href{https://developer.nvidia.com/blog/differentiable-slang-example-applications/}{Slang} (if you want you can do it in lajolla, but it's likely to be a lot of work). 

\paragraph{Stratification.}
Implement low-discrepancy sampling in lajolla.
A simpler starting point is Halton sequence.
Read the related \href{https://www.pbr-book.org/3ed-2018/Sampling_and_Reconstruction/The_Halton_Sampler}{chapters} in pbrt for the reference.
\href{https://cs.uwaterloo.ca/~thachisu/smallppm_exp.cpp}{Smallppm} from Toshiya Hachisuka also contains a low discrepenacy photon mapper using Halton sequence.

\paragraph{Bidirectional path tracing.}
Implement a bidirectional path tracer in lajolla.
Read the related \href{https://www.pbr-book.org/3ed-2018/Light_Transport_III_Bidirectional_Methods/Bidirectional_Path_Tracing}{chapters} in pbrt for the reference.
Another good reference code is \href{https://cs.uwaterloo.ca/~thachisu/smallpssmlt.cpp}{smallpssmlt} from Toshiya Hachisuka.
As a bonus, implement the efficiency-aware multiple importance sampling from Grittman et al.~\cite{Grittmann:2022:EMI}.

\paragraph{Progressive photon mapping.}
Implement progressive photon mapping~\cite{Hachisuka:2008:PPM} in lajolla.
Read the related \href{https://www.pbr-book.org/3ed-2018/Light_Transport_III_Bidirectional_Methods/Stochastic_Progressive_Photon_Mapping}{chapters} in pbrt.
Another good reference code is \href{https://cs.uwaterloo.ca/~thachisu/smallppm_exp.cpp}{smallppm} from Toshiya Hachisuka.
You can also implement the probabilistic variant from Knaus et al.~\cite{Knaus:2011:PPM}.

\paragraph{Gradient-domain path tracing.} 
Implement gradient-domain path tracing~\cite{Kettunen:2015:GPT} in lajolla.
A good reference code is \href{https://gist.github.com/BachiLi/4f5c6e5a4fef5773dab1}{smallgdpt} written by me.

\paragraph{Metropolis light transport.} Implement Kelemen-style Metropolis light transport~\cite{Kelemen:2002:SRM} in lajolla. Read the relevant \href{https://www.pbr-book.org/3ed-2018/Light_Transport_III_Bidirectional_Methods/Metropolis_Light_Transport}{chapters} in pbrt. Another good reference code is \href{https://cs.uwaterloo.ca/~thachisu/smallpssmlt.cpp}{smallpssmlt} and \href{https://cs.uwaterloo.ca/~thachisu/smallmmlt.cpp}{smallmmlt} from Toshiya Hachisuka.

\paragraph{Manifold exploration.} Implement Zeltner's specular manifold sampling~\cite{Zeltner:2020:SMS} in lajolla. Start with a basic LSDE path with reflective objects (instead of refraction).

\paragraph{Optimal multiple importance sampling.}
Implement \emph{optimal} multiple importance sampling using control variates from Kondapaneni et al.~\cite{Kondapaneni:2019:OMI} in lajolla.
Ideally, apply their method for a unidirectional path tracer or even a bidirectional path tracer (instead of just applying it for direct lighting) -- what kind of data structure do you need for maintaining the required statistics?

\paragraph{Lightcuts.}
Implement stochastic lightcuts~\cite{Yuksel:2019:SL} from Yuksel et al. in lajolla for rendering scenes with millions of lights.
As a bonus, implement the data-driven version from Wang et al.~\cite{Wang:2021:LCR}.

\paragraph{ReSTIR.}
Implement ReSTIR~\cite{Bitterli:2020:SRR} in lajolla for rendering scenes with many lights or even ReSTIR PT~\cite{Lin:2022:GRI}.
You don't need to implement the temporal reuse, but you should explore spatial reuse.

\paragraph{GPU rendering.}
Port most of lajolla to Vulkan/DirectX 12/CUDA/OpenCL/Metal/Slang.

\section{Research project ideas}
These projects are likely publishable in a conference or a journal if done well (you will likely need to work on it longer even after the course ends for this to happen though).
They are for students who are more motivated and want to get into rendering research.
If you choose to work on these projects, it is possible that they will not be finished at the end of the quarter. This is totally fine and you will still get high/full points if you show your work.
We are happy to work with you after the quarter to finish the project if you did well and are interested (and if we have the capacity).

For anything below, feel free to reach out to us for clarification about details of the project.
I did not sort them in terms of difficulty.

\paragraph{Differentiable hair rendering.}
Existing inverse hair rendering methods (e.g.,~\cite{Rosu:2022:NSL}) do not consider the light scattering in the hair fiber, and this can lead to less realistic results.
In this project, we will develop a differentiable renderer~\cite{Li:2018:DMC} that is capable of computing the derivatives of a rendering of hair with respect to the shading and geometry parameters, and use it for inferring hair reflectance and geometry from images.
You can either implement this in \href{https://github.com/mitsuba-renderer/mitsuba3}{Mitsuba 3} or \href{https://developer.nvidia.com/blog/differentiable-slang-example-applications/}{Slang} to leverage their automatic differentiation feature and GPU backends.
For the scope of the final project, it is sufficient to only consider the hair BSDF parameters and ignore the curve geometry.
To handle the geometry derivatives, you can likely adapt Bangaru et al.'s differentiable signed distance field rendering algorithm~\cite{Bangaru:2022:DRN} (since the computation of ray-curve intersection relies on the distance between the ray and the curve).

\paragraph{Differentiable curved surfaces rendering.}
Most of the discontinuity handling methods in differentiable rendering currently focuses on triangle meshes and signed distance fields. In Computer Aided Design, parametric surfaces such as NURBS or Beizer patches are often used (see Bartosz Ciechanowski's \href{https://ciechanow.ski/curves-and-surfaces/}{blog post} for an excellent introduction). Can you design a method to handle the discontinuity when differentiating curved surface rendering? Note that existing work handled this by first converting the geometry to triangle meshes~\cite{Worchel:2023:DRP}, and we are aiming for an exact method without the conversion. You may find the technique of \href{https://web.mit.edu/hyperbook/Patrikalakis-Maekawa-Cho/node68.html}{``implicitization''} useful -- it converts a polynomial parametric surface into an implicit surface, you can then apply our differentiable SDF rendering technique~\cite{Bangaru:2022:DRN}.
Same as above, you can either implement this in \href{https://github.com/mitsuba-renderer/mitsuba3}{Mitsuba 3} or \href{https://developer.nvidia.com/blog/differentiable-slang-example-applications/}{Slang} to leverage their automatic differentiation feature and GPU backends.

\paragraph{Theoretical analysis of warped-area sampling.}
While we have a sketch of correctness analysis in the warped-area sampling papers~\cite{Bangaru:2020:UWS,Bangaru:2022:DRN,Xu:2023:PSDR-WAS}, the theoretical properties of the velocity field interpolation are not very well-understood. What is the optimal interpolation? When will the integral diverge? If you choose to work on this, you likely don't need to write much code (other than to verify your intuition) and it's mostly about writing down the formalism.

\paragraph{Guided tri-directional path tracing.}
From Veach's path-space formulation, it is clear that in addition to tracing rays starting from eyes or lights, it should also be possible to trace rays starting from an arbitrary point in the scene (we explored a similar method in 2017~\cite{Anderson:2017:AED}). 
However, such \emph{tri}-directional path tracing method is difficult to apply in practice since it is difficult to decide where should we start tracing the ray. 
In this project, we will design a path guiding algorithm~\cite{Lafortune:1995:5RV,Muller:2017:PPG} that builds a 5D data structure to decide which point and direction the path should start with.
In the first phase, we will start tracing rays randomly, and record their contributions in the 5D data structure.
We will then importance sample the recorded contribution and iteratively update the data structure.
Similar methods have been applied to differentiable rendering (e.g.,~\cite{Yan:2022:EEB}), but it has not been applied in forward rendering algorithms yet.
For the scope of the final project, it is not necessary to combine your method with the camera and light subpaths.
Ultimately, it would be super cool to combine the tri-directional path tracer with VCM/UPS and use data to decide with subpath to use~\cite{Grittmann:2022:EMI}.

\paragraph{Motion-aware path guiding for animation rendering.}
Path guiding algorithms build data structures that record contributions of light paths and use them for importance sampling.
These data structures typically are fitted in a per-frame basis and do not easily extend to animation rendering. 
In this project, we will explore ways to update the path guiding data structures over time.
A potential point of reference is the neural radiance caching work from Muller et al.~\cite{Muller:2021:RNR}.
A relatively simple starting point is to implement a simple exponential weighted moving average update for the 5D tree data structure of Muller et al.~\cite{Muller:2017:PPG}.
Can you come up with heuristics to account for occlusion and parallax? What kind of information we can extract from a renderer that can help updating the data structure?

\paragraph{Jointly-learned path guiding and denoising.}
Current rendering denoisers are usually trained separately from the rendering algorithms (in particular, they are usually trained using a standard path tracer). Can we train the denoiser with the sampling algorithm together to make them compensate for each other? You might want to read Bako's Offline Deep Importance Sampling~\cite{Bako:ODI:2019}.

\paragraph{Learned multiple importance sampling.}
While optimal multiple importance sampling~\cite{Kondapaneni:2019:OMI} is shown to be optimal variance-wise, it has a few drawbacks: 1) it is computationally expensive, 2) it is only optimal under linear combination of existing contributions, and 3) it is only optimal for unbiased rendering -- it does not optimize for the Mean Square Error (bias squared plus variance). Therefore, it is tempting to simply learn to do multiple importance sampling. Take a few features as input (e.g., PDFs, roughness, curvature, ambient occlusion), train a parametric function (does not need to be a neural network) to combine your rendering samples. For this project, you don't need to implement in a renderer to start, you can just try it on some test functions.

\paragraph{Neural mutation for Metropolis light transport.}
Neural networks have been shown to be helpful for importance sampling in a Monte Carlo path tracer~\cite{Muller:2019:NIS}.
Can we apply them to Metropolis light transport as well?
Similar methods have been explored in the machine learning community for Hamiltonian Monte Carlo~\cite{Levy:2018:GHM}.
In this project, we will investigate the use of neural networks for designing mutation in a Metropolis light transport renderer.
A relatively simple starting point is to implement Levy et al.'s method in a Kelemen-style path tracer and see how well it works. 

\paragraph{Learning to build BVHs.}
Current Bounding Volume Hierarchy constructions are usually based on heuristics (in particular, surface area heuristics). Can we find a better metric through data-driven methods? Find a better BVH construction by training the construction algorithm and its parameters for many scenes to minimize the ray tracing time. You may want to read Aila et al.'s study on surface area heuristics~\cite{Aila:2013:QMB}.

\paragraph{Multiple-scattering NeRF with neural scattering fields.}
Neural Radiance Fields~\cite{Mildenhall:2021:NRS} and their variants have been revolutionizing 3D computer vision using inverse volume rendering.
However, all of the current NeRF variants do not consider multiple-scattering within the volumes.
In this project, we will explore the incorporation of multiple scattering in neural fields.
To achieve this, we need a neural representation that can represent a spatially varying, anisotropoic scattering coefficients and phase function.
We will also need a way to importance sample both the phase function and transmittance of this neural representation.
Alternatively, we can also explore a voxel-based representation (which can have similar performance to a neural representation!~\cite{KeilYu:2021:PRF}) using the SGGX phase function~\cite{Heitz:2015:SMD} and spherical harmonics for the scattering coefficients.
I would recommend starting from known lighting (e.g., an environment map) and single scattering to make the problem easier.
I would also recommend to implement this in \href{https://developer.nvidia.com/blog/differentiable-slang-example-applications/}{Slang} to leverage its automatic differentiation feature and GPU backends.

\paragraph{Spatially varying neural BSDF for neural signed distance fields.}
Another popular neural scene representation is neural signed distance fields~\cite{Bangaru:2022:DRN}.
However, similar to the situation of NeRF, existing neural signed distance fields do not model the BSDF of the surfaces (they assume all surfaces are pure emitters).
In this project, we will explore modeling a spatially varying BSDF for neural signed distance fields.
The network architecture can be a 7D coordinate network that takes a 3D position and two 2D directions and outputs the BSDF response.
For importance sampling, it might be useful to make the network a normalizing flow~\cite{Muller:2019:NIS} conditioned on the position and incoming direction. Similar to above, I would recommend starting from known lighting (e.g., an environment map) and direct lighting to make the problem easier.
I would also recommend to implement this in \href{https://developer.nvidia.com/blog/differentiable-slang-example-applications/}{Slang} to leverage its automatic differentiation feature and GPU backends.

\paragraph{Transmittance evaluation for NeRF training and inference.}
In Neural Radiance Fields, the transmittance is typically evaluated using ray marching. This is different from the ones we did in Homework 2 (e.g., ratio tracking). Implement advanced unbiased transmittance evaluation schemes (e.g., Keuttnen et al.'s unbiased ray marcher~\cite{Kettunen:2021:URT}), and compare with standard NeRF ray marching (in terms of both quality and speed). For training (i.e., derivative evaluation), you might want to implement Nimier-David et al.'s differential tracking~\cite{Nimier-David:2022:UIV}.
As usual, I would also recommend to implement this in \href{https://developer.nvidia.com/blog/differentiable-slang-example-applications/}{Slang} to leverage its automatic differentiation feature and GPU backends.

\paragraph{Faster Monte Carlo PDE solvers.}
Sawhney et al.~\cite{Sawhney:2020:MCG} recently discussed a type of PDE solver that has very similar computational patterns with rendering. Can you think of a way to make it faster by applying techniques we learned in the course? For example, would something like Metropolis light transport help?

\paragraph{Non-photorealistic rendering.}
The course so far focuses mostly on physics-inspired models. However, there is a huge space for exploration of non-physical models. How do we define the models? How do we combine with existing renderers? Read my CSE 167 \href{https://cseweb.ucsd.edu/~tzli/cse167/fa2023/lectures/17_nonphotorealistic_rendering.pdf}{slides} and our \href{https://people.csail.mit.edu/kach/dpp-dpp/}{inverse inverse rendering paper} for inspirations.

\paragraph{Faster vector graphics rendering.}
There are mainly two ways to render vector graphics currently. One is more like ray tracing~\cite{Ganacim:2014:MVG} where we shoot a ray from the pixel sample to see how many intersections it makes with the curves. One is more akin to rasterization~\cite{Kilgard:2012:GPR} where we convert the curves to triangle meshes and apply implicitization to determine whether the point is inside the shape or not. It is unclear which one is faster, or what even is the space of possible design for vector graphics rendering. Do a survey of the algorithms in this space, implement some combination of them, and ideally come up with a fast algorithm for vector graphics rendering. I find Raph Levien's \href{https://raphlinus.github.io/rust/graphics/gpu/2020/06/13/fast-2d-rendering.html}{blog post} to be very helpful.

\paragraph{Megakernel vs Wavefront.}
Laine et al. studied two styles of ray tracing on GPU back in 2013~\cite{Laine:2013:MCH}: megakernel style and wavefront style. They concluded that for complex, production material shaders, wavefront rendering is faster because of better memory locality. However, they didn't have hardware ray tracing back then and the hardware architecture is very different now. Is the conclusion still true today? Study different ways to implement path tracing on GPUs and compare their speed on modern GPU architectures.

\paragraph{Multiple-scattering LEAN or LEADR}
The normal/displacement map filtering methods LEADN/LEADR~\cite{Olano:2010:LM,Dupuy:2013:LEA} convert normal map filtering to a microfacet NDF fitting problem. However, the microfacet models they are using do not consider multiple scattering between the microfacets. It seems that it would be straightforward to apply the recent advances in multiple-scattering microfacet BSDFs to these methods. Would adding multiple-scattering into LEADN/LEADR improve the visual appearance? Do we need a different multiple-scattering model or microfacet configuration?

\bibliographystyle{plain}
\bibliography{refs}

\end{document}
